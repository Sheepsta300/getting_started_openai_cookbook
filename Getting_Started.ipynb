{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1># Getting Started with LangChain and Azure OpenAI in Python Cookbook\n",
    "\n",
    "The purpose of this notebook is to provide a step-by-step guide to getting set up with Azure OpenAI and using available models through the LangChain Framework in Python.  It will explain through examples how to implement models, embeddings, and configure the OpenAI assistant.\n",
    "</h1>"
   ],
   "id": "31d8febc6366de29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ae6c33d6694c468b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Obtaining Keys and Endpoints</h2>",
   "id": "3e40f06503d1123f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To use Azure OpenAI models, you need to obtain API keys and endpoints. You can sign up and create an Azure OpenAI resource using the following link:\n",
    "[Azure Pricing and Purchase Options](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account)"
   ],
   "id": "74086c9d0653df7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Installing and Importing Dependencies</h2>",
   "id": "35632b3fc39a7984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- First, install the most current version of LangChain and LangChain_OpenAI.\n",
    "- import libraries."
   ],
   "id": "11f05a5d0ffbd25f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "!pip install langchain\n",
    "!pip install -U langchain langchain_openai\n",
    "!pip install git+https://github.com/organization/langchain_community.git\n"
   ],
   "id": "cb0ddb73e61414f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bfc7bd1475acac69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ],
   "id": "a7711184ae6b80c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2c7da13a34ba2e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>Set all required Environment Variables</h4>\n",
    "\n",
    "To securely manage your API keys and endpoints, store them in a `.env` file and load them into your application using the `python-dotenv` library.\n",
    "\n",
    "### Step 1: Create a `.env` File\n",
    "\n",
    "Create a file named `.env` in the root directory of your project. Add your Azure OpenAI API key and endpoint to this file:\n",
    "\n",
    "AZURE_OPENAI_API_KEY=your-azure-openai-api-key\n",
    "\n",
    "AZURE_OPENAI_API_ENDPOINT=your-azure-openai-api-endpoint\n",
    "\n",
    "AZURE_API_VERSION=your-api-version\n",
    "\n",
    "Replace `your-azure-openai-api-key`, `your-azure-openai-api-endpoint`, and `your-api-version` with your actual credentials.\n",
    "\n",
    "### Step 2: Install `python-dotenv`\n",
    "\n",
    "Ensure you have the `python-dotenv` library installed. If not, install it using pip:\n",
    "\n",
    "```python\n",
    "!pip install python-dotenv\n"
   ],
   "id": "78cee2ea47ec3a1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Step 3: Load Environment Variables</h2>\n",
    "Use the following code to load the environment variables from the .env file:"
   ],
   "id": "2bf5a0da65c6e31f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Access the API key and endpoint from environment variables\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Verify that the variables are loaded\n",
    "if not openai_api_key or not azure_endpoint or not api_version:\n",
    "    raise ValueError(\"Please ensure the AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_ENDPOINT, and AZURE_API_VERSION environment variables are set\")\n"
   ],
   "id": "4367a6d78df4166c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Example Usage</h3>\n",
    "Once the environment variables are set and loaded, you can use them in your code. For instance, to create a chat model with Azure OpenAI:"
   ],
   "id": "40dfa97259706063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,  \n",
    "    azure_deployment='gpt-4o',  # Adjust this as per your deployment details\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "print(\"Chat model created successfully.\")\n"
   ],
   "id": "29e8206e03afb306"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Creating a Chat Model</h2>\n",
    "Create a chat model using the `AzureChatOpenAI` class from the LangChain library:"
   ],
   "id": "ffd59243ea2d5b3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ['AZURE_API_VERSION'],  \n",
    "    azure_deployment='gpt-4o',  # Adjust this as per your deployment details\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_API_ENDPOINT']\n",
    ")\n"
   ],
   "id": "f5cad875d368ad92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using Messages from the langchain_core.messages library allows the user to define messages for the model, as well as asign 'roles' to each of the message",
   "id": "5f3f8f3dce1d0085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from German into English\"),\n",
    "    HumanMessage(content=\"Sie haben gerade Ihr erstes Kunstliche Itelligenz Model erstellt!\"),\n",
    "]"
   ],
   "id": "a8beaa7f2c50b61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "response = model.invoke(messages)",
   "id": "c759fbd39ce713cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "response.content",
   "id": "268a90f8bc481976"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This ensures that your sensitive information like API keys and endpoints are not hard-coded in your notebook but securely managed through environment variables.",
   "id": "4fa4ea94449fa69f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2> Working with Embeddings </h2>\n",
    "Create and use embeddings with the `AzureEmbeddings` class:"
   ],
   "id": "c6a40f9048684197"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example of creating embeddings\n",
    "from langchain_openai import AzureEmbeddings\n",
    "import os\n",
    "\n",
    "# Ensure environment variables are set\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "\n",
    "if not openai_api_key or not azure_endpoint:\n",
    "    raise ValueError(\"Please set the AZURE_OPENAI_API_KEY and AZURE_OPENAI_API_ENDPOINT environment variables\")\n",
    "\n",
    "embeddings_model = AzureEmbeddings(\n",
    "    openai_api_key=openai_api_key,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "texts = [\"Artificial Intelligence is transforming the world.\", \"LangChain is a useful framework for building LLM applications.\"]\n",
    "embeddings = embeddings_model.create_embeddings(texts)\n",
    "print(embeddings)\n"
   ],
   "id": "d42cee9b31e7a370"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Configuring OpenAI Assistant with Azure OpenAI</h2>\n",
    "<h4>Configure and use the OpenAI Assistant with Azure OpenAI:</h4>"
   ],
   "id": "be2fb77233b9405f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "28a42ed860bce09d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistant\n",
    "\n",
    "assistant = OpenAIAssistant(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    "    version=os.getenv(\"AZURE_API_VERSION\") \n",
    ")\n",
    "\n",
    "response = assistant.ask(\"What is the capital of France?\")\n",
    "print(response)\n"
   ],
   "id": "a02983885b949eb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangChain Expression Language (LCEL)\n",
    "\n",
    "LangChain Expression Language (LCEL) is a powerful feature that allows developers to easily compose and customize chains of components within LangChain. This chapter introduces LCEL, provides an example of how to use it, and offers best practices for leveraging this feature effectively.\n",
    "\n"
   ],
   "id": "7991d3251f293cf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "from langchain.chains import SimpleChain  \n",
    "\n",
    "import langchain_core\n",
    "print(dir(langchain_core))\n",
    "\n",
    "# Additional imports based on the notebook examples\n",
    "from langchain_core import LangChain  \n",
    "\n",
    "# Import the LLM and Prompter components\n",
    "from langchain_core.llms import LLM\n",
    "from langchain_core.prompts import Prompter\n",
    "\n",
    "# Define your components\n",
    "llm = LLM(model_name=\"gpt-3.5-turbo\")  # Ensure the model name matches what's available\n",
    "prompter = Prompter(template=\"Translate the following text to French: {input}\")\n",
    "\n",
    "# Compose a chain\n",
    "chain = LangChain(prompter, llm)\n",
    "\n",
    "# Execute the chain with input\n",
    "result = chain.run(input=\"Hello, how are you?\")\n",
    "print(result)\n"
   ],
   "id": "420937fb597dc3aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding Memory in LangChain\n",
    "\n",
    "Memory in LangChain is essential for creating dynamic and interactive applications. By using memory, you can create agents that retain context across multiple interactions, making them more intelligent and capable of handling complex tasks. This section explores how to implement and use memory within your LangChain applications.\n"
   ],
   "id": "992d95bfc422236e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize memory and LLM\n",
    "memory = ConversationBufferMemory()\n",
    "llm = OpenAI(model_name=\"gpt-4\")\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Simulate a conversation\n",
    "response1 = conversation.predict(input=\"What is the capital of France?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = conversation.predict(input=\"Who is the president of France?\")\n",
    "print(response2)\n",
    "\n",
    "# Display memory contents\n",
    "print(\"Conversation History:\")\n",
    "print(memory.load_memory_variables({}))\n"
   ],
   "id": "774f5cadcc79abd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating Stateful Agents\n",
    "\n",
    "LangChain allows you to create agents that can maintain state across multiple interactions. This is particularly useful for chatbots, customer service applications, and any other use case where maintaining context over time is crucial. In this section, we will create a simple stateful agent using LangChain's memory capabilities.\n"
   ],
   "id": "dc51888d142c3925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from langchain.agents import Agent, ConversationBufferMemory\n",
    "from langchain.llms import OpenAI  # Adjust this import based on your actual LLM class\n",
    "\n",
    "# Initialize memory and LLM\n",
    "memory = ConversationBufferMemory()\n",
    "llm = OpenAI(model_name=\"gpt-4\")\n",
    "\n",
    "class StatefulAgent(Agent):\n",
    "    def __init__(self, llm, memory):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "\n",
    "    def ask(self, input_text):\n",
    "        # Add user message to memory\n",
    "        self.memory.add_message(\"User\", input_text)\n",
    "        \n",
    "        # Generate response using LLM\n",
    "        response = self.llm.generate(input_text)\n",
    "        \n",
    "        # Add agent's response to memory\n",
    "        self.memory.add_message(\"Agent\", response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create an instance of the stateful agent\n",
    "agent = StatefulAgent(llm=llm, memory=memory)\n",
    "\n",
    "# Interact with the agent\n",
    "response1 = agent.ask(\"What is the capital of Germany?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = agent.ask(\"Who is the Chancellor?\")\n",
    "print(response2)\n",
    "\n",
    "# Display the conversation history\n",
    "print(\"Conversation History:\")\n",
    "print(memory.get_memory())\n"
   ],
   "id": "cc5016af61f4dbfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Best Practices for Memory Management\n",
    "\n",
    "Effective memory management is key to building scalable and efficient applications with LangChain. In this section, we'll cover best practices, including memory pruning, efficient storage techniques, and tips for managing large conversation histories without degrading performance.\n"
   ],
   "id": "bd2b159a310a23de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Practical Example: Building a Customer Support Chatbot\n",
    "\n",
    "In this section, we'll put everything together to build a simple customer support chatbot. The chatbot will maintain a conversation history, allowing it to provide more relevant and contextual responses over time.\n"
   ],
   "id": "73ba461006a119b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assuming you have the necessary classes and imports\n",
    "class CustomerSupportBot(StatefulAgent):\n",
    "    def handle_query(self, query):\n",
    "        # Handle customer queries with context from previous interactions\n",
    "        response = self.ask(query)\n",
    "        return response\n",
    "\n",
    "# Initialize the customer support bot\n",
    "support_bot = CustomerSupportBot(llm=llm, memory=memory)\n",
    "\n",
    "# Example interaction\n",
    "print(support_bot.handle_query(\"I forgot my password.\"))\n",
    "print(support_bot.handle_query(\"Can you reset it for me?\"))\n",
    "print(support_bot.handle_query(\"What is the status of my last order?\"))\n",
    "\n",
    "# View the conversation history\n",
    "print(\"Full Conversation History:\")\n",
    "print(memory.get_memory())\n"
   ],
   "id": "5560cabd022e33d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d002184868959113"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
