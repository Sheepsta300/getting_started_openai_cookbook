Index: getting_started.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\r\n \"cells\": [\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"d808a92c-d36d-4291-a4ad-b12bceb62fec\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"# Getting Started OpenAi with LangChain in Python Cookbook\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"6689d50a-d463-4ad1-8968-180f93bd2f8e\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The purpose of this notebook is to provide a step-by-step guide on how to set up with OpenAI and use the available models through the LangChain framework in Python. Additionally, it will demonstrate, through examples, how to implement models and utilize available features.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"b8202de6-7d14-44e6-b378-879d26f95ce3\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Obtaining Keys and Endpoints\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"3b21c348-6a01-41d4-9df5-0d1d633367ef\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Using the [**Azure sign up page**](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account) one can sign up and create an OpenAI resource, giving access to all necessary credentials.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"ff61b81a-5c29-4da1-818f-fe3c2b1f9280\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Setup\\n\",\r\n    \"#### Installing and Importing Dependencies\\n\",\r\n    \"Microsoft [**recommends**](https://github.com/microsoft/vscode-jupyter/wiki/Installing-Python-packages-in-Jupyter-Notebooks) using:\\n\",\r\n    \"\\n\",\r\n    \"-  %pip for installing within Jupyter or IDE environments.\\n\",\r\n    \"-  %conda for installing within Conda environments.\\n\",\r\n    \" \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 21,\r\n   \"id\": \"23716db9-d8c8-4682-b662-0de8c185bd07\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"%pip install -U langchain langchain_openai\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"a54a6ef5-202f-4a81-8cbb-da6441808764\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"- Install the most current version of `langchain` and `langchain_openai`.\\n\",\r\n    \"- Import the following libraries/packages.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 2,\r\n   \"id\": \"035bd2cf-c045-4cdc-a0d1-f9bf60314f8d\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.messages import HumanMessage, SystemMessage\\n\",\r\n    \"from langchain_openai import AzureChatOpenAI\\n\",\r\n    \"import os\\n\",\r\n    \"import getpass #use getpass for entering any environment variables or keys\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"46149db2-b280-45d5-b910-df1db63040bf\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Set all required Environment Variables\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"bcf509fb-e2de-4b4c-9c55-35202af8287c\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"#API_KEY can be set as an environment variable securely like so\\n\",\r\n    \"os.environ['AZURE_OPENAI_API_KEY'] = getpass.getpass()\\n\",\r\n    \"#os.environ['AZURE_OPENAI_API_ENDPOINT'] = getpass.getpass()\\n\",\r\n    \"#os.environ['AZURE_OPENAI_API_VERSION'] = getpass.getpass()\\n\",\r\n    \"#os.environ['COMPLETIONS_MODEL'] = getpass.getpass()\\n\",\r\n    \"#...\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"4e595a7e-9b33-472d-96ff-e3229d723642\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Get all required Environment Variables\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 4,\r\n   \"id\": \"b456baeb-fe51-45c8-940e-7f6ec52da0df\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"# Setting up the deployment name\\n\",\r\n    \"DEPLOYMENT_NAME = os.environ['COMPLETIONS_MODEL']\\n\",\r\n    \"\\n\",\r\n    \"# The API key for your Azure OpenAI resource.\\n\",\r\n    \"API_KEY = os.environ['AZURE_OPENAI_API_KEY']\\n\",\r\n    \"\\n\",\r\n    \"# The base URL for your Azure OpenAI resource. e.g. \\\"https://<your resource name>.openai.azure.com\\\"\\n\",\r\n    \"ENDPOINT = os.environ['AZURE_OPENAI_API_ENDPOINT']\\n\",\r\n    \"\\n\",\r\n    \"# The API version required\\n\",\r\n    \"VERSION = os.environ['AZURE_OPENAI_API_VERSION']\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"a43b7040-d6ab-4df8-8432-907eb1ca4099\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Creating an AzureChatOpenAI Model\\n\",\r\n    \"\\n\",\r\n    \"LangChain's [**AzureChatOpenAI Info**](https://python.langchain.com/v0.2/docs/integrations/chat/azure_chat_openai/)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"172e4ff3-b3fd-4f10-bd2b-18ac1e259973\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"- Environment variables can be manually passed as parameters\\n\",\r\n    \"- The constuctor can search for environment variables of the corresponding names\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 6,\r\n   \"id\": \"74baa57a-6b35-403c-9902-2f5a9c7e2bd4\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"model = AzureChatOpenAI(\\n\",\r\n    \"    openai_api_version=VERSION,  \\n\",\r\n    \"    azure_deployment=DEPLOYMENT_NAME,\\n\",\r\n    \"    azure_endpoint=ENDPOINT,\\n\",\r\n    \"    temperature=0.5,\\n\",\r\n    \"    max_tokens=None,\\n\",\r\n    \"    timeout=60,\\n\",\r\n    \"    max_retries=2,\\n\",\r\n    \"    # organization=\\\"...\\\",\\n\",\r\n    \"    # model=\\\"gpt-35-turbo\\\",\\n\",\r\n    \"    # model_version=\\\"0125\\\",\\n\",\r\n    \"    # other params...\\n\",\r\n    \"    )\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"23b80bbe-442c-4374-a885-87bd6408dc28\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"**AZURE_OPENAI_API_VERSION** and **AZURE_OPENAI_API_ENDPOINT** are both being passed, but **AZURE_OPENAI_API_KEY** is being retrieved within the constructor.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"9b85766e-2581-4f24-86c1-cde10a0601c0\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Other Optional Parameters\\n\",\r\n    \"\\n\",\r\n    \"- `temperature` determines how creative and unpredictable, or how deterministic and predictable, the model should be in its responses. A temperature of 0 would be predictable, while anything higher would make responses more random.\\n\",\r\n    \"\\n\",\r\n    \"- `max_tokens` defines the maximum number of tokens (words or pieces of words) the model can generate in its response.\\n\",\r\n    \"\\n\",\r\n    \"- `timeout` specifies the maximum amount of time (in seconds) to wait for a response from the API before timing out.\\n\",\r\n    \"\\n\",\r\n    \"- `max_retries` sets the number of times the API request should be retried in case of failure before giving up.\\n\",\r\n    \"\\n\",\r\n    \"- `organization` is used to specify the organization ID if required for API access.\\n\",\r\n    \"\\n\",\r\n    \"- `model` specifies the model to be used.\\n\",\r\n    \"\\n\",\r\n    \"- `model_version` indicates the specific version of the chosen model to use.\\n\",\r\n    \"\\n\",\r\n    \"- See the [**API Reference**](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html) for more details\\n\",\r\n    \"\\n\",\r\n    \"- Other parameters may be available in different SDK's\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"28a397b3-27b7-49f0-bccf-108f39196ee6\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Model Usage\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"1a152eab-52e5-48f3-bc8f-0b410d65c7d0\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Using Messages from the `langchain_core.messages` Library\\n\",\r\n    \"\\n\",\r\n    \"The `langchain_core.messages` library allows the user to define messages for the model and assign roles to each message.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"259a462a-4a83-4f17-a337-be04d67d3020\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"- LangChain-compatible chat models take a list of `messages` as `input` and return the AI message as `output`.\\n\",\r\n    \"\\n\",\r\n    \"- All messages have `role` and `content` properties. However, below the roles are set using `SystemMessage` and `HumanMessage`. We'll cover more on this later.\\n\",\r\n    \"\\n\",\r\n    \"- Additional provider-specific information can be incorporated using the `Additional Properties`.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 16,\r\n   \"id\": \"57c6f2ae-b6d0-46e3-831a-ad5f77daf0bd\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"messages = [\\n\",\r\n    \"    SystemMessage(content=\\\"Translate the following from German into English\\\"),\\n\",\r\n    \"    HumanMessage(content=\\\"Sie haben gerade Ihr erstes Kunstliche Itelligenz Model erstellt!\\\"),\\n\",\r\n    \"]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 18,\r\n   \"id\": \"b6a40126-bede-46dc-8922-8721ac2f9c22\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"response = model.invoke(messages)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 20,\r\n   \"id\": \"876d40ef-a75f-4d62-bc0a-43de6b55ce30\",\r\n   \"metadata\": {\r\n    \"scrolled\": true\r\n   },\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'You have just created your first artificial intelligence model!'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 20,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"a0c37fe3-ed5d-4633-8e97-79cdba0e0365\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Prompting\\n\",\r\n    \"\\n\",\r\n    \"- Prompts are the inputs to language models, refined from raw user inputs to be ready for processing by the models.\\n\",\r\n    \"\\n\",\r\n    \"- [Prompting](https://www.datacamp.com/tutorial/prompt-engineering-with-langchain) involves crafting text inputs that clearly communicate with the models, outlining the specific task we want it to accomplish. This can include:\\n\",\r\n    \"    - Selecting the appropriate wording and setting a particular tone or style.\\n\",\r\n    \"    - Providing necessary context.\\n\",\r\n    \"    - Assigning a role, such as asking it to respond as if it were a native speaker of a certain language.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"2ee1ff1f-dbfc-445f-807e-4682db59460f\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Prompt Templates\\n\",\r\n    \"\\n\",\r\n    \"- LangChain allows developers to design parameterized [**Prompt Templates**](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/) that are reusable and easily transferable between different models for integration.\\n\",\r\n    \"\\n\",\r\n    \"- It takes user input and inserts said input into the prompt to feed into the language models.\\n\",\r\n    \"\\n\",\r\n    \"#### `PromptTemplate`\\n\",\r\n    \"\\n\",\r\n    \"`PromptTemplate` is used to create an instance of [**Prompt**](https://python.langchain.com/v0.1/docs/expression_language/get_started/#1-prompt), and this is `invoked` by sending it to a model, which produces a `PromptValue`.\\n\",\r\n    \"\\n\",\r\n    \"The example code uses `.from_template`, which handles a single string template with placeholders for dynamic inputs.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 97,\r\n   \"id\": \"04a11bb8-e4d9-4687-ab89-210aa1b7b51b\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.prompts import PromptTemplate  \\n\",\r\n    \"\\n\",\r\n    \"prompt_template = PromptTemplate.from_template(\\n\",\r\n    \"    \\\"What vege crops can I grow in {month} in {city}, New Zealand?\\\"\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"prompt_value = prompt_template.format(month = \\\"December\\\", city = \\\"Rotorua\\\")\\n\",\r\n    \"\\n\",\r\n    \"# print(prompt_template) # <- uncomment to see\\n\",\r\n    \"# print(prompt_value) # <- uncomment to see\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 39,\r\n   \"id\": \"f92db0c1-3741-42be-9d1e-edbe5010b61e\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'In Rotorua, New Zealand, December falls at the start of summer, which is a great time for growing a variety of vegetables. Here are some vegetable crops you can consider planting in December:\\\\n\\\\n1. **To'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 39,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"response.content[:200]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"1ca904f4-faf1-4b95-bf77-7c7a7fa64515\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### `ChatPromptTemplate`\\n\",\r\n    \"\\n\",\r\n    \"This is optimized for a conversation-like format. The prompt is a list of chat messages. Each chat message is associated with `role` and `content`. In the example code, `.from_messages` is used to include multiple messages.\\n\",\r\n    \"\\n\",\r\n    \"Here, we will hardcode roles in the chat prompt, as opposed to using `SystemMessage` or `HumanMessage` like earlier.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 93,\r\n   \"id\": \"46e869bf-370d-4742-837f-c7d5eb76a891\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.prompts import ChatPromptTemplate\\n\",\r\n    \"\\n\",\r\n    \"chat_template = ChatPromptTemplate.from_messages(\\n\",\r\n    \"    [\\n\",\r\n    \"        (\\\"system\\\", \\\"\\\"\\\"\\n\",\r\n    \"                    You are a travel agent helping customers plan their trips.\\n\",\r\n    \"                    Recommend them regarding popular Accommodation, Food and Activities of the country customer is asking.\\n\",\r\n    \"                    \\\"\\\"\\\"), \\n\",\r\n    \"        (\\\"ai\\\", \\\"Hi there, What can I help you with today?\\\"),\\n\",\r\n    \"        (\\\"human\\\", \\\"Hi I'm {name}, I'm planning a trip to {country}. Any recommendations\\\")\\n\",\r\n    \"    ]\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"prompt_value = chat_template.format_messages(name=\\\"Lucy\\\", country=\\\"New Zealand\\\")\\n\",\r\n    \"\\n\",\r\n    \"# print(chat_template) # <- uncomment to see\\n\",\r\n    \"# print(prompt_value) # <- uncomment to see\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 43,\r\n   \"id\": \"c87adc53-1248-441d-aa46-44f410c24796\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"\\\"Hi Lucy! New Zealand is a fantastic destination with stunning landscapes, rich culture, and plenty of activities to keep you entertained. Here are some recommendations for your trip:\\\\n\\\\n### Accommodation\\\\n1. **Auckland:**\\\\n   - **SkyCity Hotel:** Located in the heart of the city, close to major attractions.\\\\n   - **Cordis, Auckland:** A luxurious option with excellent amenities.\\\\n\\\\n2. **Queenstown:**\\\\n   - **Eichardt's Private Hotel:** A luxury hotel with breathtaking views of Lake Wakatipu.\\\\n   - **The Rees Hotel:** Offers a mix of hotel rooms and apartments with stunning lake views.\\\\n\\\\n3. **Rotorua:**\\\\n   - **Solitaire Lodge:** A luxury lodge offering stunning lake views and fine dining.\\\\n   - **Regent of Rotorua:** A boutique hotel with modern amenities.\\\\n\\\\n### Food\\\\n1. **Auckland:**\\\\n   - **Depot Eatery & Oyster Bar:** Known for its fresh seafood and casual vibe.\\\\n   - **Clooney:** Offers a fine dining experience with contemporary New Zealand cuisine.\\\\n\\\\n2. **Wellington:**\\\\n   - **Logan Brown:** A fine\\\"\"\r\n      ]\r\n     },\r\n     \"execution_count\": 43,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"response.content[:1000]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"cb6794d6-ba1f-4367-bae6-af0136212713\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Assigning Roles Using LangChain Messages\\n\",\r\n    \"\\n\",\r\n    \"Compared to hardcoding the roles like above, LangChain Messages allow for more flexibility and better management, especially with complex conversations involving multiple roles. It also simplifies the visualization of the conversation flow.\\n\",\r\n    \"\\n\",\r\n    \"It is therefore recommended to use LangChain messages where possible.\\n\",\r\n    \"\\n\",\r\n    \"**Basic Message Types**\\n\",\r\n    \"\\n\",\r\n    \"|             |                                                                 |\\n\",\r\n    \"|-------------|-----------------------------------------------------------------|\\n\",\r\n    \"| `SystemMessage` | Set how the AI should behave (appropriate wording, tone, style, etc.) |\\n\",\r\n    \"| `HumanMessage`  | Message sent from the user                                      |\\n\",\r\n    \"| `AIMessage`     | Message from the AI chat model (context setting, guidance for response) |\\n\",\r\n    \"\\n\",\r\n    \"For more info, see [**Message Types**](https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/) and [**API Reference**](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages).\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"95152680-ebc2-4106-99bf-679547d0d1fe\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### `base message` and `MessagePromptTemplate`\\n\",\r\n    \"We can also pass a `base message` or `MessagePromptTemplate` instead of tuples.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 95,\r\n   \"id\": \"b956d584-881a-4e9b-ae81-565ffdd7bf4a\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.prompts import HumanMessagePromptTemplate\\n\",\r\n    \"\\n\",\r\n    \"chat_template = ChatPromptTemplate.from_messages(\\n\",\r\n    \"    [\\n\",\r\n    \"        SystemMessage(\\n\",\r\n    \"            content=(\\\"You are a translator. You are to translate the text into English.\\\"\\n\",\r\n    \"            )\\n\",\r\n    \"        ),\\n\",\r\n    \"        HumanMessagePromptTemplate.from_template(\\\"{text}\\\"),\\n\",\r\n    \"    ]\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"prompt_value = chat_template.format_messages(text=\\\"アサヒスーパードライは日本のビールのです。\\\")\\n\",\r\n    \"\\n\",\r\n    \"# print(chat_template) # <- uncomment to see\\n\",\r\n    \"# print(prompt_value) # <- uncomment to see\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 20,\r\n   \"id\": \"648579a0-ca55-4885-bf70-47c0e08e066d\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'Asahi Super Dry is a Japanese beer.'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 20,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"33f05d01-151b-4cda-a386-567a5523bb03\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### `MessagePlaceHolder`\\n\",\r\n    \"This  is used to select which messages to include when formatting.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 121,\r\n   \"id\": \"0f7c266b-4438-4912-9b21-b0611960f296\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.prompts import (\\n\",\r\n    \"    HumanMessagePromptTemplate,\\n\",\r\n    \"    SystemMessagePromptTemplate,\\n\",\r\n    \"    ChatPromptTemplate,\\n\",\r\n    \"    MessagesPlaceholder,\\n\",\r\n    \"    AIMessagePromptTemplate,\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"# SYSTEM ROLE Prompt\\n\",\r\n    \"system_template = SystemMessagePromptTemplate.from_template(\\\"\\\"\\\"\\n\",\r\n    \"                                            You are a precise assistant who knows the shcedule of the team.\\n\",\r\n    \"                                            Schedule details are as follows: {schedule}.\\n\",\r\n    \"                                            Only provide information to the team members.\\n\",\r\n    \"                                            Strictly only provide information specific to what is asked, Do not give extra information.\\n\",\r\n    \"                                            \\\"\\\"\\\")\\n\",\r\n    \"# HUMAN ROLE Prompt\\n\",\r\n    \"human_template = HumanMessagePromptTemplate.from_template(\\\"My name is {user_name}.\\\")\\n\",\r\n    \"# AI ROLE Prompt\\n\",\r\n    \"ai_template = AIMessagePromptTemplate.from_template(\\\"Hello {user_name}, how can I help you today?\\\")\\n\",\r\n    \"\\n\",\r\n    \"chat_prompt = ChatPromptTemplate.from_messages(\\n\",\r\n    \"    [\\n\",\r\n    \"        # this has essentially created a 'conversation history'\\n\",\r\n    \"        system_template,\\n\",\r\n    \"        human_template,\\n\",\r\n    \"        ai_template,\\n\",\r\n    \"        MessagesPlaceholder(variable_name=\\\"conversation\\\"), \\n\",\r\n    \"    ]\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"# print(chat_prompt) # <- uncomment to see the chat prompt \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"6ab8c223-97d6-4a5c-83a5-24eeba7c47f3\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"We can then input more prompts, which will take the `MessagePlaceholders`' place and create lines of sentences or a conversation.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 124,\r\n   \"id\": \"cea2fce5-2692-4df1-bac1-80bb9472d7f0\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \" from langchain_core.messages import HumanMessage, AIMessage\\n\",\r\n    \"\\n\",\r\n    \"schedule = \\\"\\\"\\\"\\n\",\r\n    \"    Team Members: Alice, Bob, Carol, David, Emily\\n\",\r\n    \"    Team Meeting Schedule: Every Tuesday at 11:00 AM\\n\",\r\n    \"    Topic: LangChain with Azure OpenAI Integration\\n\",\r\n    \"\\\"\\\"\\\"\\n\",\r\n    \"# these messages will take MESSAGEPLACEHOLDERS place\\n\",\r\n    \"human_query = HumanMessage(\\\"When is the next team meeting and who is attending?\\\")\\n\",\r\n    \"ai_message = AIMessage(\\\"Hold on a second, let me check the schedule for you.\\\")\\n\",\r\n    \"\\n\",\r\n    \"prompt_value = chat_prompt.format_messages(\\n\",\r\n    \"    conversation=[human_query, ai_message], user_name=\\\"David\\\", schedule=schedule\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"# print(prompt_value) # <- uncomment to see the prompt \"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 126,\r\n   \"id\": \"1558cfbf-5806-4a24-a0fa-8a1fb90a00d8\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'The next team meeting is on Tuesday at 11:00 AM. The attendees are Alice, Bob, Carol, David, and Emily.'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 126,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"34ec2ed1-83f1-4331-97fb-a6319f53e1fd\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### `FewShotPrompt`\\n\",\r\n    \"\\n\",\r\n    \"We can use examples (shots) to condition the model for a better response by including some example input and output in the prompt. This will inform the model about the context and how we want the output to be formatted.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 128,\r\n   \"id\": \"fcaa0701-6f2c-4ffb-b011-75b6849d2ebe\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"Currency Unit Conversion: [Input] one dollar => [Output] $1\\n\",\r\n      \"Currency Unit Conversion: [Input] one hundred yen => [Output] ¥100\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"from langchain_core.prompts import FewShotPromptTemplate\\n\",\r\n    \"\\n\",\r\n    \"examples = [\\n\",\r\n    \"    {\\\"input\\\": \\\"one dollar\\\", \\\"output\\\": \\\"$1\\\"},\\n\",\r\n    \"    {\\\"input\\\": \\\"thirty five euros\\\", \\\"output\\\": \\\"€35\\\"},\\n\",\r\n    \"]\\n\",\r\n    \"\\n\",\r\n    \"example_prompt = PromptTemplate(\\n\",\r\n    \"    input_variables=[\\\"input\\\", \\\"output\\\"], template=\\\"Currency Unit Conversion: [Input] {input} => [Output] {output}\\\"\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"# unpack the first example dictionary and feed it to the prompt template to format\\n\",\r\n    \"print(example_prompt.format(**examples[0]))\\n\",\r\n    \"\\n\",\r\n    \"# feed examples to FewShotPromptTemplate to generate a final prompt\\n\",\r\n    \"fewshot_prompt = FewShotPromptTemplate(\\n\",\r\n    \"    examples=examples,\\n\",\r\n    \"    example_prompt=example_prompt,\\n\",\r\n    \"    suffix= \\\"Convert the currency units: {input}\\\",\\n\",\r\n    \"    input_variables=[\\\"input\\\"],\\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"prompt_value= fewshot_prompt.format(input=\\\"one hundred yen\\\")\\n\",\r\n    \"\\n\",\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"print(response.content)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"e52b27f1-90a9-46c7-8a43-a8ae356235ef\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Chaining\\n\",\r\n    \"\\n\",\r\n    \"- Many LangChain components implement the [**Runnable**](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface) protocol, which allows them to be easily chained together. These components can be combined in a sequence of calls, which we refer to as a chain.\\n\",\r\n    \"\\n\",\r\n    \"- Chaining `Runnables` in sequence.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 131,\r\n   \"id\": \"7bfdc351-1901-4fc4-bceb-067a68b54a30\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"system_template = SystemMessagePromptTemplate.from_template(\\\"\\\"\\\"\\n\",\r\n    \"    You are an expert in {country} cuisine. \\n\",\r\n    \"    Keep it simple and short.\\n\",\r\n    \"    \\\"\\\"\\\")\\n\",\r\n    \"\\n\",\r\n    \"chat_prompt = ChatPromptTemplate.from_messages(\\n\",\r\n    \"    [\\n\",\r\n    \"         system_template,\\n\",\r\n    \"        (\\\"human\\\", \\\"I'd like to find out about {country} cuisine.\\\"),\\n\",\r\n    \"        (\\\"human\\\", \\\"{question}\\\"),\\n\",\r\n    \"    ]\\n\",\r\n    \")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"3f6682cc-576c-45f0-8b5b-52674a0f30a8\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"This is how we have been using prompts, but now we will skip this step and invoke using the chain.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 135,\r\n   \"id\": \"761b48c5-92e5-4857-b280-cdd486c29b02\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'In Japan, the most popular sashimi is often maguro (tuna), particularly the fatty cuts like otoro. Globally, salmon sashimi tends to be more popular due to its rich flavor and buttery texture.'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 135,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"prompt_value = chat_prompt.format_messages(country=\\\"Japanese\\\", question=\\\"What is the most popular Sashimi in Japan vs the rest of the world?\\\")\\n\",\r\n    \"\\n\",\r\n    \"response = model.invoke(prompt_value)\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 136,\r\n   \"id\": \"3893edc8-acc0-418e-ac5d-31a65b00b8b8\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"In Japan, the most popular sashimi is often maguro (tuna), particularly the fatty part known as toro. Globally, salmon sashimi tends to be more popular due to its rich flavor and widespread availability.\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"from langchain_core.output_parsers import StrOutputParser\\n\",\r\n    \"\\n\",\r\n    \"chain = chat_prompt | model | StrOutputParser()\\n\",\r\n    \"\\n\",\r\n    \"print(\\n\",\r\n    \"    chain.invoke(\\n\",\r\n    \"        {\\n\",\r\n    \"            \\\"country\\\": \\\"Japanese\\\", \\n\",\r\n    \"            \\\"question\\\": \\\"What is the most popular Sashimi in Japan vs the rest of the world?\\\"\\n\",\r\n    \"        }\\n\",\r\n    \"    )\\n\",\r\n    \")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"c4a85fe6-ee18-4fc8-88c2-b67297ac8ef8\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Check the costs and token usage of a given model API call\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 21,\r\n   \"id\": \"561dfbfb-0933-4650-8230-23f1bc2b1f46\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain.callbacks import get_openai_callback\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 25,\r\n   \"id\": \"c9081591-0b58-44b3-89c7-c3a512f84f4c\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"messages = [\\n\",\r\n    \"    SystemMessage(content=\\\"Translate the following from German into English\\\"),\\n\",\r\n    \"    HumanMessage(content=\\\"What's the first planet in the Solar System?\\\"),\\n\",\r\n    \"]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"61c9a128-f702-4f48-b695-436629e5ac1a\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"`get_openai_callback()` is a context manager of the [**OpenAICallbackHandler**](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/callbacks/openai_info.py) class, meaning it calls this class and creates an instance when used.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 28,\r\n   \"id\": \"178153e8-c37c-4ce2-bbb4-15ec6feb58f6\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"Total tokens used: 37\\n\",\r\n      \"Total prompt tokens: 27\\n\",\r\n      \"Total prompt tokens: 10\\n\",\r\n      \"Total cost (in dollars): $0.000285\\n\",\r\n      \"Total successful requests): 1\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"with get_openai_callback() as cb:\\n\",\r\n    \"    model.invoke(messages)\\n\",\r\n    \"    print(f\\\"Total tokens used: {cb.total_tokens}\\\")\\n\",\r\n    \"    print(f\\\"Total prompt tokens: {cb.prompt_tokens}\\\")\\n\",\r\n    \"    print(f\\\"Total prompt tokens: {cb.completion_tokens}\\\")\\n\",\r\n    \"    print(f\\\"Total cost (in dollars): ${cb.total_cost}\\\")\\n\",\r\n    \"    print(f\\\"Total successful requests): {cb.successful_requests}\\\")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"6b82390d-bdc4-4b92-bb52-77580cca48f9\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"### Async model usage\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 31,\r\n   \"id\": \"089dc929-599b-46a7-8882-3c1f3bed190f\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"messages = [\\n\",\r\n    \"    SystemMessage(content=\\\"Give a two sentance explanation of the following python code\\\"),\\n\",\r\n    \"    HumanMessage(content=\\\"await model.ainvoke(messages)\\\"),\\n\",\r\n    \"]\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 33,\r\n   \"id\": \"44ccd78b-47e7-462e-8bf0-8c00899c4ea7\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'This Python code snippet uses the `await` keyword to asynchronously call the `ainvoke` method on the `model` object with `messages` as its argument. The `await` keyword indicates that the code execution will pause until the `ainvoke` method completes, allowing other tasks to run concurrently.'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 33,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"response = await model.ainvoke(messages)\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"ab9705c6-0512-48b3-b117-8e1bc385cf86\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## Tools and Structured output\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"e62770fa-c0e4-45ad-8f3b-5fd64e835857\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Import the required packages. `BaseModel` is a parent class that all tools will inherit from, and `Field` is used to define all properties of the tool.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 36,\r\n   \"id\": \"84527cdd-6d45-4d18-a3c4-887a8537c9a5\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from langchain_core.pydantic_v1 import BaseModel, Field\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"e7b68b06-ccd1-40f1-9201-9814de191254\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Tools are essentially classes that can be passed to a chosen model to influence or structure how the response should be formatted or generated.\\n\",\r\n    \"\\n\",\r\n    \"**For example:**\\n\",\r\n    \"\\n\",\r\n    \"- A Weather tool with a specific API call could be passed so the model knows to use this specific API for data retrieval.\\n\",\r\n    \"- A City tool with fields like `population`, `size`, and `main_language` so the model can return any city-related queries with an object containing the corresponding filled fields.\\n\",\r\n    \"- An Image tool with a `url` field to be returned when asked to search for an image containing a dog, with the field containing the URL of the image.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 40,\r\n   \"id\": \"05f0ff77-07bd-4041-9ab5-c2ba13ca644c\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from typing import Optional\\n\",\r\n    \"#adds the ability to make a field Optional [NULL]\\n\",\r\n    \"\\n\",\r\n    \"class Person(BaseModel):\\n\",\r\n    \"    '''Information about a given person'''\\n\",\r\n    \"\\n\",\r\n    \"    name: str = Field(..., description=\\\"The name of a person\\\")\\n\",\r\n    \"    alive: bool = Field(..., description=\\\"Thether the person is alive or not\\\")\\n\",\r\n    \"    profession: str = Field(..., description=\\\"What the person does for work or professionally\\\")\\n\",\r\n    \"    noteable_features: str = Field(..., description=\\\"Any noteworthy features/achievements about the person\\\")\\n\",\r\n    \"    hobbies: Optional[str] = Field(description=\\\"Anything hobbies the person may have\\\")\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 42,\r\n   \"id\": \"c5d9b7cc-d127-4f28-ab8f-829f236ef419\",\r\n   \"metadata\": {\r\n    \"scrolled\": true\r\n   },\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"Person(name='Kate Sheppard', alive=False, profession='Suffragist', noteable_features=\\\"Leader of the women's suffrage movement in New Zealand, instrumental in making New Zealand the first country to grant women the right to vote in 1893.\\\", hobbies=None)\"\r\n      ]\r\n     },\r\n     \"execution_count\": 42,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"structured_model = model.with_structured_output(Person)\\n\",\r\n    \"response = structured_model.invoke(\\\"Tell me about Kate Sheppard\\\")\\n\",\r\n    \"response\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"52eb76bf-283c-4095-aa33-27e772f09292\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"##### As the response of the invocation has been structured using the Person tool, the response can be accessed like a `Person` object.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 44,\r\n   \"id\": \"63f05dc8-7832-429c-b710-ceb72a56b492\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"Kate Sheppard\\n\",\r\n      \"False\\n\",\r\n      \"Suffragist\\n\",\r\n      \"Leader of the women's suffrage movement in New Zealand, instrumental in making New Zealand the first country to grant women the right to vote in 1893.\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"print(response.name)\\n\",\r\n    \"print(response.alive)\\n\",\r\n    \"print(response.profession)\\n\",\r\n    \"print(response.noteable_features)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"4d5247e4-c810-4a61-98ee-9534ae10d300\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### JSON\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"350506c9-8d59-43c5-a5ba-cc2493da2faf\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Models can also be explicitly told to respond in a JSON structured format. This could then be used for future API calls or for easier access to information. However, **the word \\\"json\\\" must be included in the message string.**\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 49,\r\n   \"id\": \"42478d15-0aa2-4e47-b279-e807397d9f31\",\r\n   \"metadata\": {\r\n    \"scrolled\": true\r\n   },\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'{\\\\n  \\\"name\\\": \\\"Jane Doe\\\",\\\\n  \\\"alive\\\": true,\\\\n  \\\"profession\\\": \\\"Software Engineer\\\"\\\\n}'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 49,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"json_model = model.bind(response_format={\\\"type\\\": \\\"json_object\\\"})\\n\",\r\n    \"\\n\",\r\n    \"response = json_model.invoke(\\n\",\r\n    \"    '''Return a JSON object of a random person with features like name,\\n\",\r\n    \"    alive (if they're alive or not) and their profession.''' \\n\",\r\n    \")\\n\",\r\n    \"\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"35208928-b640-4b77-9640-0b8f12b2924f\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The response can then be formatted into a JSON object and accessed using normal JSON notation.\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 52,\r\n   \"id\": \"c59bbe45-28c7-4aba-abfc-9d5e63aac3f7\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"{'name': 'Jane Doe', 'alive': True, 'profession': 'Software Engineer'}\"\r\n      ]\r\n     },\r\n     \"execution_count\": 52,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"person = json.loads(response.content)\\n\",\r\n    \"person\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"535b4a4c-b435-40d1-9145-3918661fff7d\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"## File input\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"8d150455-42c3-4943-89fa-d916ef76060b\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Models can be fed files of various forms as their inputs.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"cd54937c-eae4-4adc-be21-3779b6281e01\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Images\\n\",\r\n    \"\\n\",\r\n    \"Import the required libraries to allow for HTTP requests and file conversion.\\n\",\r\n    \"\\n\",\r\n    \"- Make sure the file format is correct; otherwise, a \\\"File not found\\\" error will be thrown.\\n\",\r\n    \"\\n\",\r\n    \"- Ensure the HTTP request was successful with a 200 response code; otherwise, a 404 error will be thrown.\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 58,\r\n   \"id\": \"ebd83bd8-da43-4c2b-a07f-a1bd0ecd0d19\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"name\": \"stdout\",\r\n     \"output_type\": \"stream\",\r\n     \"text\": [\r\n      \"The status code is: 200\\n\"\r\n     ]\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"import base64\\n\",\r\n    \"import httpx\\n\",\r\n    \"\\n\",\r\n    \"url = \\\"https://upload.wikimedia.org/wikipedia/commons/b/bf/Aoraki_Mount_Cook.JPG\\\"\\n\",\r\n    \"\\n\",\r\n    \"# Use an HTTP get request to retrieve the image\\n\",\r\n    \"response = httpx.get(url)\\n\",\r\n    \"\\n\",\r\n    \"# Check the request was successful\\n\",\r\n    \"if response.status_code == 200:\\n\",\r\n    \"    # Extract the binary image data\\n\",\r\n    \"    binary_image_data = response.content\\n\",\r\n    \"\\n\",\r\n    \"    # Encode the binary_image_data into base 64 so that it's in bytes\\n\",\r\n    \"    # This allows it to be transmitted in JSON or text format\\n\",\r\n    \"    byte_image_data = base64.b64encode(binary_image_data)\\n\",\r\n    \"\\n\",\r\n    \"    # Decode the data into UTF-8 to allow for to make the data workable\\n\",\r\n    \"    # Web applications and models require it in a workable format\\n\",\r\n    \"    # UTF-8 is a fairly standardised format\\n\",\r\n    \"    image_data = byte_image_data.decode(\\\"utf-8\\\")\\n\",\r\n    \"\\n\",\r\n    \"print(f'The status code is: {response.status_code}')\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"65f20898-73d6-4fed-9433-bb086019f1a7\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"When using data of different types, such as text in the `SystemMessage` and a file in the `HumanMessage`, it's necessary to specify a type header so the model knows how to interpret the data.\\r\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 60,\r\n   \"id\": \"fdb153be-bcb0-4a61-902a-3394cfb1206d\",\r\n   \"metadata\": {},\r\n   \"outputs\": [\r\n    {\r\n     \"data\": {\r\n      \"text/plain\": [\r\n       \"'This image depicts a stunning mountainous landscape with a snow-capped peak reflected in a calm, clear lake. The mountains are rugged and covered in snow, suggesting a cold and possibly remote location. The clear blue sky and the reflection in the water add to the serene and picturesque quality of the scene. The foreground features some rocks along the edge of the lake, enhancing the natural beauty of the setting. This type of scenery is often found in high-altitude regions, such as the Himalayas, the Alps, or the Southern Alps in New Zealand.'\"\r\n      ]\r\n     },\r\n     \"execution_count\": 60,\r\n     \"metadata\": {},\r\n     \"output_type\": \"execute_result\"\r\n    }\r\n   ],\r\n   \"source\": [\r\n    \"messages = [  \\n\",\r\n    \"    SystemMessage(content=[{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"describe the image location\\\"}]),\\n\",\r\n    \"    HumanMessage(\\n\",\r\n    \"    content=[\\n\",\r\n    \"        {\\n\",\r\n    \"            \\\"type\\\": \\\"image_url\\\",\\n\",\r\n    \"            \\\"image_url\\\": {\\\"url\\\": f\\\"data:image/jpeg;base64,{image_data}\\\"},\\n\",\r\n    \"        },\\n\",\r\n    \"    ]\\n\",\r\n    \"    )\\n\",\r\n    \"]\\n\",\r\n    \"response = model.invoke(messages)\\n\",\r\n    \"response.content\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"d0e97f80-aa24-4ff8-af52-0f5b955a2090\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"#### Audio\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"dc44d1e7-7051-411b-8f9d-65f6548bb1a0\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"Chat completion models are somewhat limited in how they work with audio files. Audio files cannot be transmitted to the model directly but must instead be transcribed using the [**Whisper API**](https://platform.openai.com/docs/guides/speech-to-text). The transcribed text can then be used with the model.\\r\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"0c2fcba8-2d47-40ff-84c2-079250568a6e\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \"The Whisper API can currently only be accessed through `langchain_community` services directly; there is no `langchain` support for Python at the moment. Therefore, either the `langchain_community` or `openai` library must be included.\\r\\n\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": 4,\r\n   \"id\": \"dc1f2f70-b45e-4bdb-bc62-d723a3c3e2e7\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": [\r\n    \"from openai import AzureOpenAI\\n\",\r\n    \"from langchain_community.document_loaders.parsers import OpenAIWhisperParser\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"markdown\",\r\n   \"id\": \"a9e25294-7755-4a0f-8669-cbbb48de6a74\",\r\n   \"metadata\": {},\r\n   \"source\": [\r\n    \" URL of the audio file to be retrieved\\n\",\r\n    \"audio_url = \\\"https://www.doc.govt.nz/globalassets/documents/conservation/native-animals/birds/bird-song/takahe-song-10.mp3\\\"\\n\",\r\n    \"\\n\",\r\n    \" Use an HTTP GET request to retrieve the audio data\\n\",\r\n    \"audio_test_file = httpx.get(audio_url).content\\n\",\r\n    \"\\n\",\r\n    \"#byte_audio_data = base64.b64encode(binary_audio_data)\\n\",\r\n    \"    \\n\",\r\n    \" Decode the base64 data into a UTF-8 string\\n\",\r\n    \"#audio_test_file = byte_audio_data.decode(\\\"utf-8\\\")\\n\",\r\n    \"\\n\",\r\n    \"client = AzureOpenAI(\\n\",\r\n    \"    api_key=os.environ[\\\"OPENAI_API_KEY\\\"],  \\n\",\r\n    \"    api_version=os.environ[\\\"OPENAI_API_VERSION\\\"],\\n\",\r\n    \"    azure_endpoint = os.environ[\\\"OPENAI_API_ENDPOINT\\\"]\\n\",\r\n    \")\\n\",\r\n    \"    \\n\",\r\n    \"deployment_id = os.environ[\\\"COMPLETIONS_MODEL\\\"] #This will correspond to the custom name you chose for your deployment when you deployed a model.\\\"\\n\",\r\n    \"#audio_test_file = \\\"Real_Estate_Audio.mp3\\\"\\n\",\r\n    \"    \\n\",\r\n    \"result = client.audio.transcriptions.create(\\n\",\r\n    \"    file=audio_test_file,            \\n\",\r\n    \"    model='<WHISPER_MODEL'>\\n\",\r\n    \")\\n\",\r\n    \"    \\n\",\r\n    \"print(result)\"\r\n   ]\r\n  },\r\n  {\r\n   \"cell_type\": \"code\",\r\n   \"execution_count\": null,\r\n   \"id\": \"848faf78-a219-430c-91d9-b4d70c3e0655\",\r\n   \"metadata\": {},\r\n   \"outputs\": [],\r\n   \"source\": []\r\n  }\r\n ],\r\n \"metadata\": {\r\n  \"kernelspec\": {\r\n   \"display_name\": \"Python 3 (ipykernel)\",\r\n   \"language\": \"python\",\r\n   \"name\": \"python3\"\r\n  },\r\n  \"language_info\": {\r\n   \"codemirror_mode\": {\r\n    \"name\": \"ipython\",\r\n    \"version\": 3\r\n   },\r\n   \"file_extension\": \".py\",\r\n   \"mimetype\": \"text/x-python\",\r\n   \"name\": \"python\",\r\n   \"nbconvert_exporter\": \"python\",\r\n   \"pygments_lexer\": \"ipython3\",\r\n   \"version\": \"3.11.8\"\r\n  }\r\n },\r\n \"nbformat\": 4,\r\n \"nbformat_minor\": 5\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/getting_started.ipynb b/getting_started.ipynb
--- a/getting_started.ipynb	(revision 7ad5190ae7916d0e01c1df4de64aa7bcefd5cfe0)
+++ b/getting_started.ipynb	(date 1723766935384)
@@ -1121,9 +1121,7 @@
    "cell_type": "markdown",
    "id": "65f20898-73d6-4fed-9433-bb086019f1a7",
    "metadata": {},
-   "source": [
-    "When using data of different types, such as text in the `SystemMessage` and a file in the `HumanMessage`, it's necessary to specify a type header so the model knows how to interpret the data.\r\n"
-   ]
+   "source": "When using data of different types, such as text in the `SystemMessage` and a file in the `HumanMessage`, it's necessary to specify a type header so the model knows how to interpret the data.\n"
   },
   {
    "cell_type": "code",
@@ -1170,17 +1168,13 @@
    "cell_type": "markdown",
    "id": "dc44d1e7-7051-411b-8f9d-65f6548bb1a0",
    "metadata": {},
-   "source": [
-    "Chat completion models are somewhat limited in how they work with audio files. Audio files cannot be transmitted to the model directly but must instead be transcribed using the [**Whisper API**](https://platform.openai.com/docs/guides/speech-to-text). The transcribed text can then be used with the model.\r\n"
-   ]
+   "source": "Chat completion models are somewhat limited in how they work with audio files. Audio files cannot be transmitted to the model directly but must instead be transcribed using the [**Whisper API**](https://platform.openai.com/docs/guides/speech-to-text). The transcribed text can then be used with the model.\n"
   },
   {
    "cell_type": "markdown",
    "id": "0c2fcba8-2d47-40ff-84c2-079250568a6e",
    "metadata": {},
-   "source": [
-    "The Whisper API can currently only be accessed through `langchain_community` services directly; there is no `langchain` support for Python at the moment. Therefore, either the `langchain_community` or `openai` library must be included.\r\n"
-   ]
+   "source": "The Whisper API can currently only be accessed through `langchain_community` services directly; there is no `langchain` support for Python at the moment. Therefore, either the `langchain_community` or `openai` library must be included.\n"
   },
   {
    "cell_type": "code",
